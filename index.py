from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. Texto base
texto_base = """
A intelig√™ncia artificial (IA) √© um campo da ci√™ncia da computa√ß√£o dedicado √† cria√ß√£o de sistemas que podem realizar tarefas normalmente associadas √† intelig√™ncia humana. Alguns exemplos incluem reconhecimento de fala, vis√£o computacional, tomada de decis√µes e tradu√ß√£o de idiomas.
A IA moderna depende fortemente de algoritmos de aprendizado de m√°quina, onde os computadores aprendem com grandes volumes de dados.
"""

# 2. Dividir texto em chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
docs = text_splitter.create_documents([texto_base])

# 3. Criar embeddings usando modelo HuggingFace local
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# 4. Criar o banco vetorial
vectorstore = FAISS.from_documents(docs, embedding_model)

# 5. Criar o modelo LLM local com Ollama
llm = OllamaLLM(model="llama3.1:8b") # <- Mudar conforme LLM instalada no seu ambiente 

# 6. Pipeline de Perguntas e Respostas
qa = RetrievalQA.from_chain_type(
  llm=llm,
  chain_type="stuff",
  retriever=vectorstore.as_retriever(),
  return_source_documents=True
)

question = "Quais s√£o os exemplos de aplica√ß√µes de intelig√™ncia artificial?"
# question = "Quem √© o principal criador da Linguagem Artificial" # Pergunta que n√£o tem haver com a fonte
response = qa.invoke(question)


print("\nüìå Pergunta:", question)
print("\n‚úÖ Resposta:", response['result'])
print("\nüìö Fontes:")
for doc in response["source_documents"]:
  print("-", doc.page_content)
